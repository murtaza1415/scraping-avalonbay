Before submission:
    Copy essenstial files to submissison folder. (execute.py, python_files, local_config, input, schedule_tasks)
    Delete unecessary files.
    Change variables.
        (default concurrency: 3)
    Create zip file with version number.
    Test.

pending:
    (TODO) All other fancy features. Check past scrapes to see if there are any missing features.
    (TODO) Pyinstaller.
    (TODO) Ability to send emails to multiple addresses. (use either gmail or rentalbeast's private smtp)

avalonbay_v1.4:
    (done) Update the report on Avalon to include existing apartments as well. 
	(eone) Write report in output folder.

avalonbay_v1.3:
    (done) Change time format.
    (done) Outlook removing line breaks in email.
    (done) Remove extra usernames and passwords from email_.py

avalonbay_v1.2:
    (done) Fix date format.
    (done) Remove apartment name from json.
    (done) Remove apartment id from both json and csv.
    (done) Don't add package name to unit_details.
    (done) Move furnished price to unit_details.
    (done) floor_plan_name is missing
    (done) Email on success. Includes apartment and unit count.
    (done) Email on missing apartments.
        ensure website mode only.
    (done) Combine all emails except error email.
    (done) Remove extra log statements.
    (done) Comments and cleaning.
    (done) Batch files for mode 1, 4, 6.


avalonbay_v1.1:
    (done) Include apartment name in email when new apartment gets added.
    (done) Put a counter on the units.
    (done) Json files should only be deleted after user selects a mode.


avalonbay_v1.0:
    (done) 5 Modes: Website, state name, State/city file, input url, scrape by a file (a list of apt id and urls)
    (done) Download images.
        Download only once.
        Image name should be unique across all websites.
        (test) Get community data first (small scrape) and share with Robert.
        Fix addresses.
        Remove duplicate rows.
    (done) Reduce network usage. Abort unnecessary requests.
    (done) Verify integrity of data.
    (done) Retries.
	(done) Exception handling.
    (done) Delete old json files when scraping.
    (done) Remove unecessary sleeps and waits.
    (done) Remove unecessary log statements.
    (done) Scrape all important fields.
    (test) Output format:
        Follow JSON example.
        comm url, comm name, comm address, the obvious stuff, Furnish price, specials, details
        mm-dd-yyyy
    (test) Concurrency
        Concurrency for download_images, get_community?
    (test) check for 2 prices and sqft?
    (test) Write community dict to community csv file.
    (test) Proxies.
        test. How frequently should I rotate the proxy? Per city, and per community.
        test. Use rentalbeast project level file for proxies.
    (test) Log file.
        Log file size.
        Remove unecessary logging.
    (test) Config file for email, concurrency
    (test) Change data format.
    (test) Do not scrape same community multiple times.
    (test) Master file on states/cities. Only scrape states/cities mentioned in the file.
    (done) Pip installations from script.
    (done) Notifications:
        done. Empty file notification
        done. Error notifications. (Audible alarm not required, send email)
        done. Notify report on newly added communities.


Misc:
    - Json to csv converter: https://conversiontools.io/convert/xml-to-excel
    - Home page: https://www.avaloncommunities.com/
    - Locations: https://www2.avaloncommunities.com/apartment-locations
    - Community example: https://www.avaloncommunities.com/california/berkeley-apartments/
    - Each entry listed on the community page is a unit.
    - In apt-number, first number shows phase, second number shows unit.






async def get_community_without_opening(page, community, proxy):
    max_attempts = 2
    for attempt_number in range(1,max_attempts+1):
        try:
            community_url = community['community_url']
            community_name = community['community_name']
            community_state = community['address_state']
            community_city = community['address_city']

            community_data = [
                {
                    "additional_data": {
                        "original_url": community_url,
                        "apartment_name": community_name,
                        "apartment_id": "pending"
                    },
                    "apartment_address_data": {
                        "city": community['address_city'],
                        "state": community['address_state'],
                        "street_name": community['address_street'],
                        "street_number": community['address_number'],
                        "zip_code": community['address_zip']
                    },
                    "contact_information": {
                        "Office Hours": "pending",
                        "name": "pending",
                        "phone": "pending"
                    },
                    "listings": [],
                    "error": None
                }
            ]
            
            apartments_button = page.locator('xpath=//button[@id="apartment-toggle"]')

            await apartments_button.click()

            embedded_script = await page.locator('xpath=//script[@id="fusion-metadata"]').inner_html()
            json_start_index = embedded_script.index('{"itemsCount":')
            json_end_index = embedded_script.index(',"communityFilters"')
            embedded_json = embedded_script[json_start_index:json_end_index]
            embedded_json = json.loads(embedded_json)

            
            # TODO: Make sure that the matching of unit with community is reliable.
            unit_cards = page.locator(f'xpath=//div[@class="ant-card-body"][.//a[contains(@href,"{community_url}")]]')

            num_units = await unit_cards.count()

            logging.info(f'Num of units: {num_units}')

            for i in range(num_units):
                unit_card = unit_cards.nth(i)
                title = await unit_card.locator('xpath=.//div[@class="ant-card-meta-title"]').inner_text()
                unit_number = title.split('\n')[0].replace('Apt.', '',1).strip()
                
                unit_specs = await unit_card.locator('xpath=.//div[@class="description"]').inner_text()
                unit_specs = unit_specs.split('â€¢')
                
                unit_price = await unit_card.locator('xpath=.//span[contains(@class,"unit-price")]').inner_text()
                unit_url = await unit_card.locator('xpath=.//a[contains(@class,"unit-item-details-title")]').get_attribute('href')
                unit_url = unit_url.split('?')[0]
                
                unit_furnish_price = None
                furnish_div = unit_card.locator('xpath=.//div[contains(text(),"Furnished starting at")]')
                if await furnish_div.is_visible():
                    unit_furnish_price = await furnish_div.inner_text()
                    unit_furnish_price = unit_furnish_price.replace('Furnished starting at','').strip()

                unit_img_url = await unit_card.locator('xpath=.//div[contains(@class,"unit-image")]//img').first.get_attribute('src')
                unit_img_url = parse.urljoin(community_url, unit_img_url)  # Some image urls like '/pf/resources/img/notfound-borderless.png?d=80' need joining.
                unit_img_filename = get_image_filename(unit_img_url)

                '''
                unit_adate = None
                url_split = unit_url.split('&')
                for s in url_split:
                    if 'moveInDate' in s:
                        s = s.replace('%2F','/')
                        unit_adate = s.split('=')[1]
                        break
                '''

                for unit_json in embedded_json['items']:
                    if unit_json['unitName'] == unit_number:
                        unit_beds = unit_json['bedroomNumber']
                        if unit_beds:    # Convert from integer to string.
                            unit_beds = str(unit_beds)
                        unit_baths = unit_json['bathroomNumber']
                        if unit_baths:    # Convert from integer to string.
                            unit_baths = str(unit_baths)
                        unit_sqft = unit_json['squareFeet']
                        if unit_sqft:    # Convert from integer to string.
                            unit_sqft = str(unit_sqft)
                        unit_floorplan_name = None
                        if 'floorplan' in unit_json:
                            unit_floorplan_name = unit_json['floorPlan']['name']
                            unit_floorplan_name = unit_floorplan_name.split('-')[0]
                        unit_adate = None
                        if unit_json['furnishStatus'] == 'Designated':        # Furnished only apartments do not have unfurnished adate.
                            unit_adate = unit_json['availableDateFurnished']
                            unit_adate = unit_adate.split('T')[0]    # Remove time.
                        else:
                            unit_adate = unit_json['availableDateUnfurnished']
                            unit_adate = unit_adate.split('T')[0]    # Remove time.
                        unit_specials = []
                        if 'promotions' in unit_json:
                            for promo in unit_json['promotions']:
                                promo_title = promo['promotionTitle']
                                unit_specials.append(promo_title)
                        unit_specials = '\n'.join(s for s in unit_specials)
                        unit_package = None
                        if 'finishPackage' in unit_json:
                            package_name = unit_json['finishPackage']['name']
                            package_disc = unit_json['finishPackage']['description']
                            unit_package = package_name + ': ' + package_disc
                        unit_virtual = None
                        if 'virtualTour' in unit_json:
                            unit_virtual = unit_json['virtualTour']['space']
                        break

                logging.info('\n')
                logging.info(f'Unit no: {unit_number}')
                logging.info(f'Floorplan: {unit_floorplan_name}')
                logging.info(f'Spec list: {unit_specs}')
                logging.info(f'Beds: {unit_beds}')
                logging.info(f'Baths: {unit_baths}')
                logging.info(f'Sqft: {unit_sqft}')
                logging.info(f'Price: {unit_price}')
                logging.info(f'Fur price: {unit_furnish_price}')
                logging.info(f'Url: {unit_url}')
                logging.info(f'Image url: {unit_img_url}')
                logging.info(f'Virtual tour: {unit_virtual}')
                logging.info(f'Move in: {unit_adate}')
                logging.info(f'Specials: {unit_specials}')
                logging.info(f'Packages: {unit_package}')

                community_data[0]['listings'].append(
                    {
                        "available": unit_adate,
                        "bathrooms": unit_baths,
                        "bedrooms": unit_beds,
                        "floor_plan_name": unit_floorplan_name,
                        "rent": unit_price,
                        "furnishedRent": unit_furnish_price,
                        "sqft": unit_sqft,
                        "unitId": unit_number,
                        "unit_url": unit_url,
                        "image_url": unit_img_url,
                        "image_filename": unit_img_filename,
                        "virtual_tour": unit_virtual,
                        "specials": unit_specials,
                        "unit_details": unit_package
                    }
                )

                output_file_name = slugify(f'{community_state}_{community_city}_{community_name}').replace('-', '_') + '.json'
                with open(Path(this_directory, f'output/{output_file_name}'), 'w', encoding='utf-8') as f:
                    json.dump(community_data, f)

                await download_image(unit_img_url, proxy)

            scraped_communities.append(community_url)
            
            break
        except:
            community_url = community['community_url']
            logging.info(f'Exception in get_community: {community_url}')
            logging.info(f'Exception in attempt {attempt_number}')
            logging.exception('exception: ')










# Send email notification for newly added apartments to '_avalonbay_apartments.csv'.
# global newly_added_communities
# num_newly_added = len(newly_added_communities)
# if num_newly_added > 0:
#     logging.info(f'\n{num_newly_added} apartments were newly added. Notifying via email.')
#     new_apartments_str = ''.join((e + '\n') for e in newly_added_communities)
#     time_now = datetime.now().strftime("%m/%d/%Y, %H:%M:%S")
#     email_subject = 'AvalonBay Scraper - New apartments were added.'
#     email_body = f'The following {num_newly_added} apartments have been newly added to "_avalonbay_apartments.csv"\n\n{new_apartments_str}\nTime of event: {time_now}'
#     send_email(email_subject, email_body)



# # Send email notification for empty files.
# if (error_state == False) and (len(empty_files) > 0):    # If error_state is true, do not send a separate email for empty files.
#     logging.info('\nSome files are empty...')
#     logging.info('Sending email notification...')
#     empty_files_str = ''.join((e + '\n') for e in empty_files)
#     time_now = datetime.now().strftime("%m/%d/%Y, %H:%M:%S")
#     email_subject = 'AvalonBay Scraper - Empty csv/json files.'
#     email_body = f'The following files were created during scraping but they are empty: \n\n{empty_files_str}\nTime of event:  {time_now}'
#     send_email(email_subject, email_body)

# logging.info('\n')
# logging.info(scraped_communities)
# missing_communities = get_missing_communities('1')
# missing_communities_str = ''.join((e + '\n') for e in missing_communities)
# logging.info('\n')
# logging.info(missing_communities_str)